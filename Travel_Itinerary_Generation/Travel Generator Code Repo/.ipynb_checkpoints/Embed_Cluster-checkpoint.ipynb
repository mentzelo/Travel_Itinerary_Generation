{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0037b0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def embed_d2v(path, vector_size=100, window=5, min_count=2, workers=4):\n",
    "    #### We need to select the vector size(how large should our vector be to capture data), window(number of context words before/behind each token)\n",
    "    #### min_count(ignore all words with frequency lower than this) and workers(how many worker threads to train model)\n",
    "    #### Here we read in the CSV file which has rows as reviews, and columns as review attributes(text, photos etc)\n",
    "    df = pd.read_csv( path, low_memory = False)\n",
    "    \n",
    "    #### We now group reviews that are from the same location together and concatenate them together to form long form text\n",
    "    grouped_reviews = df.groupby('placeInfo/name')\n",
    "    concatenated_reviews = grouped_reviews['text'].apply(lambda x: ' '.join(x))\n",
    "    reviews_by_entity = pd.DataFrame(concatenated_reviews)\n",
    "    reviews_by_entity.index = reviews_by_entity.index.get_level_values('placeInfo/name')\n",
    "    df = reviews_by_entity\n",
    "    \n",
    "    \n",
    "    \n",
    "    col = 'text'\n",
    "    # Prepare a list of tagged documents for Doc2Vec\n",
    "    documents = [TaggedDocument(doc.split(), [col + '_' + str(idx)]) for idx, doc in df[col].iteritems()]\n",
    "\n",
    "    # Train a Doc2Vec model for this column\n",
    "    model = Doc2Vec(documents, vector_size = vector_size, window = window, min_count = min_count, workers = workers, epochs=40)  # Adjust hyperparameters as needed\n",
    "\n",
    "    # Embed each document in the column\n",
    "    column_embeddings = [model.infer_vector(doc.words) for doc in documents]\n",
    "\n",
    "    # Add a new column for the embeddings\n",
    "    df[col + '_embeddings'] = column_embeddings\n",
    "    \n",
    "    return df, model\n",
    "\n",
    "def kmeans_cluster(df, embedding_column, n_clusters, random_state):\n",
    "    #### Specify the df column in which the embeddings are located e.g. df['text_embeddings']\n",
    "    #### n_clusters, and random state\n",
    "    # Extract embeddings as a NumPy array\n",
    "    embeddings = np.vstack(embedding_column.tolist()) \n",
    "    embeddings = normalize(embeddings)\n",
    "    # Calculate cosine similarity matrix (1 - cosine distance)\n",
    "    dist_matrix = 1 - squareform(pdist(embeddings, metric='cosine'))\n",
    "\n",
    "\n",
    "    ## K means below for example\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)  # Set a random state for reproducibility\n",
    "    clusters = kmeans.fit_predict(embeddings)\n",
    "\n",
    "\n",
    "    # Assign cluster labels to the DataFrame\n",
    "    df['cluster'] = clusters\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def kmeans_evaluate(df, max_clusters, max_iter):\n",
    "    #### We evaluate different numbers of clusters by their inertia\n",
    "    sse = {}\n",
    "    for k in range(1, max_clusters):\n",
    "        kmeans = KMeans(n_clusters=k, max_iter=max_iter).fit(embeddings)\n",
    "        df[\"cluster\"] = kmeans.labels_\n",
    "        #print(data[\"clusters\"])\n",
    "        sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\n",
    "    plt.figure()\n",
    "    plt.plot(list(sse.keys()), list(sse.values()))\n",
    "    plt.xlabel(\"Number of clusters\")\n",
    "    plt.ylabel(\"SSE\")\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "def vec_similarity(text, model):\n",
    "    \n",
    "    #### Function to give you top 10 document tags and their cosine similarity\n",
    "    #### text is a string, model is the model\n",
    "    \n",
    "    tokens = text.split()\n",
    "    new_vector = model.infer_vector(tokens)\n",
    "    sims = model.dv.most_similar([new_vector])\n",
    "\n",
    "    return sims\n",
    "\n",
    "\n",
    "\n",
    "def find_matches(df, group, rating_column):\n",
    "    ##This function inputs a specific component name, and dataframe with components and their respective cluster, ratings, and returns \n",
    "    ## a list of components in the same cluster, ordered by rating. \n",
    "    ## cluster column  = name of column that has cluster(usually 'cluster'),\n",
    "    \n",
    "\n",
    "    #group = df.loc[df['name'] == name, 'cluster'].iloc[0]  # Get the group of the given location\n",
    "    other_locations = df.loc[(df['cluster'] == group)]  # Filter for other locations in the same group\n",
    "    other_locations = other_locations.sort_values(by=rating_column, ascending=False)\n",
    "    return list(zip(other_locations['name'].tolist(), other_locations[rating_column].tolist()))\n",
    "    \n",
    "def get_components(days, prefs, df, factor):\n",
    "    ##Prefs are a list of the numerical clusters representing a user's preferences, days is the length of trip in days, \n",
    "    ## factor is the scale factor on how many components per day we want to generate.\n",
    "    ## max number of components is the maximum of either days*factor, or number of clusters selected by user\n",
    "    max_components = max(days*factor, len(prefs))\n",
    "    components = []\n",
    "    top_components = {}\n",
    "    #Creates a dictionary with keys = cluster #, and values are ordered list of components by rating\n",
    "    for i in prefs:\n",
    "        top_components[i] = ec.find_matches(df, i, 'numberOfReviews') ####Can use rawRanking which is Tripadvisor quality measure,\n",
    "        ## Or can use numberOfReviews which is a proxy for popularity\n",
    "    #We iterate through our dictionary adding one representative from each cluster at a time to our final components list until\n",
    "    #the list has reached our maximum\n",
    "    j = 0\n",
    "    while len(components) < max_components:\n",
    "        for i in prefs:\n",
    "            if top_components[i][j][0] not in components: ###add index 0 if you want to leave out the raw rating in output\n",
    "                components.append(top_components[i][j][0]) ###add index 0 if you want to leave out the raw rating in output\n",
    "            else:\n",
    "                j = j+1\n",
    "                components.append(top_components[i][j][0])  ###add index 0 if you want to leave out the raw rating in output\n",
    "    return components\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
